{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy\n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 4)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "def suppress_vocals(audio_file, bass_cutoff=300, sr=22050):\n",
    "    y, sr = librosa.load(audio_file, sr=sr, mono=False)\n",
    "    # print(sr)\n",
    "    cutoff_freq = bass_cutoff\n",
    "\n",
    "    # Get the filter coefficients for a Butterworth highpass filter\n",
    "    nyquist_freq = 0.5 * sr\n",
    "    cutoff_normalized = cutoff_freq / nyquist_freq\n",
    "    b, a = butter(1, cutoff_normalized, btype='highpass')\n",
    "\n",
    "    # Apply the filter to the audio signal\n",
    "    y_filtered = filtfilt(b, a, y)\n",
    "\n",
    "    bass = y - y_filtered\n",
    "\n",
    "    # Separate left and right channels\n",
    "    left_channel = y_filtered[0]\n",
    "    right_channel = y_filtered[1]\n",
    "\n",
    "    # Subtract higher frequency signals from left and right channels\n",
    "    vocals_removed_left = left_channel - right_channel\n",
    "    vocals_removed_right = right_channel - left_channel\n",
    "\n",
    "    # # Add the bass signal back to the instrumental signal\n",
    "    instrumental_audio_left = vocals_removed_left + bass[0]\n",
    "    instrumental_audio_right = vocals_removed_right + bass[1]\n",
    "    # instrumental_audio = vocals_removed_left + bass[0]\n",
    "\n",
    "    instrumental_audio = np.vstack([instrumental_audio_left, instrumental_audio_right])\n",
    "\n",
    "    # instrumental_audio, _ = librosa.effects.hpss(instrumental_audio)\n",
    "\n",
    "    return librosa.to_mono(instrumental_audio), sr\n",
    "\n",
    "def extract_features(y, sr):\n",
    "    zcr = librosa.zero_crossings(y).sum()\n",
    "\n",
    "    energy = scipy.linalg.norm(y)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y,sr=sr,n_fft=64)[0,0]\n",
    "    return [zcr, energy,spec_cent]\n",
    "\n",
    "def extract_and_normalize_features(y, sr=22050):\n",
    "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, delta=0.04, wait=4)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "    onset_samples = librosa.frames_to_samples(onset_frames)\n",
    "    # print(onset_samples)\n",
    "    frame_sz = sr * 0.020\n",
    "    list = []\n",
    "    for i in onset_samples:\n",
    "        slice = y[i : i + int(frame_sz)]\n",
    "        # window = np.hamming(len(slice))\n",
    "        list.append(extract_features(slice, sr))\n",
    "    features = np.array(list)\n",
    "    min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    features_scaled = min_max_scaler.fit_transform(features)\n",
    "    # print(features)\n",
    "    return features_scaled\n",
    "\n",
    "def plot_features(features):\n",
    "    plt.scatter(features[:,0], features[:,2])\n",
    "    plt.xlabel('Zero Crossing Rate (scaled)')\n",
    "    plt.ylabel('Spectral Centroid (scaled)')\n",
    "    plt.show()\n",
    "\n",
    "def plot_kmeans_for_different_values_directly(data, max_number_of_instruments=5):\n",
    "    inertias = []\n",
    "    silhouette_avg = []\n",
    "\n",
    "    for i in range(2,max_number_of_instruments):\n",
    "        kmeans = KMeans(n_clusters=i)\n",
    "        kmeans.fit(data)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_avg.append(silhouette_score(data, kmeans.labels_))\n",
    "\n",
    "    plt.plot(range(2,max_number_of_instruments), inertias, marker='o')\n",
    "    plt.title('Elbow method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(2,max_number_of_instruments),silhouette_avg)\n",
    "    plt.xlabel('Values of K') \n",
    "    plt.ylabel('Silhouette score') \n",
    "    plt.title('Silhouette analysis For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "def predict_number_of_instruments(data, max_number_of_instruments=6, threshold=1.5):\n",
    "    inertias = []\n",
    "    silhouette_avg = []\n",
    "\n",
    "    for i in range(2,max_number_of_instruments):\n",
    "        kmeans = KMeans(n_clusters=i)\n",
    "        kmeans.fit(data)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_avg.append(silhouette_score(data, kmeans.labels_))\n",
    "\n",
    "    if (inertias[0] / inertias[1]) < threshold: return 1\n",
    "    return np.argmax(silhouette_avg) + 2\n",
    "\n",
    "    # model = sklearn.cluster.AffinityPropagation(preference=-1.5, damping=0.9)\n",
    "    # labels = model.fit_predict(features_scaled)\n",
    "    # return np.max(labels) - 1\n",
    "    \n",
    "\n",
    "def find_true_number_of_instruments(audio_file):\n",
    "    split_file = audio_file.split('.')\n",
    "    split_file[-1] = 'txt'\n",
    "    txt_file = '.'.join(split_file)\n",
    "\n",
    "    non_blank_count = sum(1 for line in open(txt_file) if line.strip())\n",
    "\n",
    "    return non_blank_count\n",
    "\n",
    "## Testing\n",
    "\n",
    "audio_file = ''\n",
    "\n",
    "y, sr = suppress_vocals(audio_file)\n",
    "\n",
    "Audio(data=y, rate=sr)\n",
    "\n",
    "test_files = glob.glob(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testin for case when there is more than one instrument present in audio excerpt\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "print(len(test_files))\n",
    "X = []\n",
    "Y = []\n",
    "for test_file in test_files:\n",
    "    true = find_true_number_of_instruments(test_file)\n",
    "    y, sr = suppress_vocals(test_file, sr=22050)\n",
    "    features_scaled = extract_and_normalize_features(y, sr)\n",
    "    X.append(features_scaled)\n",
    "    pred = predict_number_of_instruments(features_scaled)\n",
    "    Y.append(true)\n",
    "    total_count += 1\n",
    "    if true == pred: correct_count +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
